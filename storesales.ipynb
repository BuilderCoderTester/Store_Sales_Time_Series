{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12850175,"sourceType":"datasetVersion","datasetId":8127483}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, OrdinalEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import FunctionTransformer, power_transform\nfrom sklearn.linear_model import LinearRegression, SGDRegressor, ElasticNet, Ridge\nfrom sklearn.ensemble import BaggingRegressor, ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor, VotingRegressor\nfrom sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T04:55:41.790450Z","iopub.execute_input":"2025-08-26T04:55:41.790780Z","iopub.status.idle":"2025-08-26T04:55:41.796349Z","shell.execute_reply.started":"2025-08-26T04:55:41.790757Z","shell.execute_reply":"2025-08-26T04:55:41.795437Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"# Load the Data","metadata":{}},{"cell_type":"code","source":"\n    train = pd.read_csv('/kaggle/input/time-series-store-sales/train.csv')\n    test = pd.read_csv('/kaggle/input/time-series-store-sales/test.csv')\n    holiday = pd.read_csv('/kaggle/input/time-series-store-sales/holidays_events.csv')\n    oil = pd.read_csv('/kaggle/input/time-series-store-sales/oil.csv')\n    store = pd.read_csv('/kaggle/input/time-series-store-sales/stores.csv')\n    transaction = pd.read_csv('/kaggle/input/time-series-store-sales/transactions.csv')\n    sample = pd.read_csv('/kaggle/input/time-series-store-sales/sample_submission.csv')\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T04:55:41.797641Z","iopub.execute_input":"2025-08-26T04:55:41.797845Z","iopub.status.idle":"2025-08-26T04:55:43.276480Z","shell.execute_reply.started":"2025-08-26T04:55:41.797831Z","shell.execute_reply":"2025-08-26T04:55:43.275654Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# ===== DATE PROCESSING FUNCTIONS =====\ndef modify_date(df):\n    \"\"\"Enhanced date feature engineering\"\"\"\n    df = df.copy()\n    df['date'] = pd.to_datetime(df['date'])\n    \n    # Basic date features\n    df['year'] = df['date'].dt.year\n    df['month'] = df['date'].dt.month\n    df['day'] = df['date'].dt.day\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['weekofyear'] = df['date'].dt.isocalendar().week\n    df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)\n    \n    # Additional time features for better time series modeling\n    df['quarter'] = df['date'].dt.quarter\n    df['is_month_start'] = df['date'].dt.is_month_start.astype(int)\n    df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n    df['days_from_start'] = (df['date'] - df['date'].min()).dt.days\n    \n    # Cyclical encoding for better ML performance\n    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n    df['dayofweek_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n    df['dayofweek_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n    \n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T04:55:43.277176Z","iopub.execute_input":"2025-08-26T04:55:43.277390Z","iopub.status.idle":"2025-08-26T04:55:43.283864Z","shell.execute_reply.started":"2025-08-26T04:55:43.277374Z","shell.execute_reply":"2025-08-26T04:55:43.283250Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# ===== HOLIDAY PROCESSING =====\ndef process_holidays(df, holiday):\n    \"\"\"Improved holiday processing\"\"\"\n    # Filter out transferred holidays\n    holidays_clean = holiday[holiday[\"transferred\"] == False].copy()\n    \n    # Process different holiday types\n    nat = holidays_clean[holidays_clean[\"locale\"] == \"National\"][[\"date\", \"description\"]].rename(\n        columns={\"description\": \"holiday_national\"})\n    reg = holidays_clean[holidays_clean[\"locale\"] == \"Regional\"][[\"date\", \"locale_name\", \"description\"]].rename(\n        columns={\"locale_name\": \"state\", \"description\": \"holiday_regional\"})\n    loc = holidays_clean[holidays_clean[\"locale\"] == \"Local\"][[\"date\", \"locale_name\", \"description\"]].rename(\n        columns={\"locale_name\": \"city\", \"description\": \"holiday_local\"})\n    \n    # Merge holidays step by step\n    df_merged = df.merge(nat, on=\"date\", how=\"left\")\n    df_merged = df_merged.merge(reg, on=[\"date\", \"state\"], how=\"left\")\n    df_merged = df_merged.merge(loc, on=[\"date\", \"city\"], how=\"left\")\n    \n    # Create holiday indicators\n    df_merged['is_national_holiday'] = df_merged['holiday_national'].notna().astype(int)\n    df_merged['is_regional_holiday'] = df_merged['holiday_regional'].notna().astype(int)\n    df_merged['is_local_holiday'] = df_merged['holiday_local'].notna().astype(int)\n    df_merged['is_any_holiday'] = (df_merged['is_national_holiday'] | \n                                   df_merged['is_regional_holiday'] | \n                                   df_merged['is_local_holiday']).astype(int)\n    \n    return df_merged","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T04:55:43.285536Z","iopub.execute_input":"2025-08-26T04:55:43.285789Z","iopub.status.idle":"2025-08-26T04:55:43.302784Z","shell.execute_reply.started":"2025-08-26T04:55:43.285772Z","shell.execute_reply":"2025-08-26T04:55:43.302022Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# ===== EXTERNAL DATA PROCESSING =====\ndef process_oil_data(df, oil):\n    \"\"\"Process and merge oil price data\"\"\"\n    oil_clean = oil.copy()\n    oil_clean['date'] = pd.to_datetime(oil_clean['date'])\n    \n    # Forward fill missing oil prices\n    oil_clean['dcoilwtico'] = oil_clean['dcoilwtico'].fillna(method='ffill')\n    oil_clean['dcoilwtico'] = oil_clean['dcoilwtico'].fillna(method='bfill')\n    \n    # Create oil price features\n    oil_clean['oil_price_ma7'] = oil_clean['dcoilwtico'].rolling(window=7).mean()\n    oil_clean['oil_price_change'] = oil_clean['dcoilwtico'].pct_change()\n    df['date'] = pd.to_datetime(df['date'])\n    return df.merge(oil_clean, on='date', how='left')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T04:55:43.303437Z","iopub.execute_input":"2025-08-26T04:55:43.303648Z","iopub.status.idle":"2025-08-26T04:55:43.319456Z","shell.execute_reply.started":"2025-08-26T04:55:43.303632Z","shell.execute_reply":"2025-08-26T04:55:43.318744Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"def process_transactions(df, transaction):\n    \"\"\"Process and merge transaction data\"\"\"\n    trans_clean = transaction.copy()\n    trans_clean['date'] = pd.to_datetime(trans_clean['date'])\n    \n    # Create transaction features\n    trans_clean['transactions_ma7'] = trans_clean.groupby('store_nbr')['transactions'].transform(\n        lambda x: x.rolling(window=7, min_periods=1).mean())\n    \n    return df.merge(trans_clean, on=['date', 'store_nbr'], how='left')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T04:55:43.320197Z","iopub.execute_input":"2025-08-26T04:55:43.320443Z","iopub.status.idle":"2025-08-26T04:55:43.337002Z","shell.execute_reply.started":"2025-08-26T04:55:43.320418Z","shell.execute_reply":"2025-08-26T04:55:43.336307Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# ===== ENCODING FUNCTIONS =====\nexisting_cat_cols = []\ndef onehotencoding(df, columns):\n    for col in columns:\n        existing_cat_cols.append(col)\n    \n    if not existing_cat_cols:\n        print(\"not working\")\n        return df\n    \n    trf1 = ColumnTransformer(\n        transformers=[\n            ('OneHotEncode', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), existing_cat_cols)\n        ],\n        remainder='passthrough'\n    )\n    \n    transformed = trf1.fit_transform(df)\n    \n    # Get feature names\n    ohe_features = trf1.named_transformers_['OneHotEncode'].get_feature_names_out(existing_cat_cols)\n    remain_columns = [col for col in df.columns if col not in existing_cat_cols]\n    \n    combine_features = list(ohe_features) + remain_columns\n    \n    return pd.DataFrame(transformed, columns=combine_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T04:55:43.337850Z","iopub.execute_input":"2025-08-26T04:55:43.338093Z","iopub.status.idle":"2025-08-26T04:55:43.355559Z","shell.execute_reply.started":"2025-08-26T04:55:43.338078Z","shell.execute_reply":"2025-08-26T04:55:43.354794Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# ===== DATA TRANSFORMATION =====\ndef data_distribution(df, choice, target_col='sales'):\n    \"\"\"Improved data transformation with target preservation\"\"\"\n    df_copy = df.copy()\n    \n    # Separate target if it exists\n    target_data = None\n    if target_col in df_copy.columns:\n        target_data = df_copy[target_col].copy()\n        df_copy = df_copy.drop(columns=[target_col])\n    \n    # Only transform numeric columns\n    numeric_cols = df_copy.select_dtypes(include=[np.number]).columns\n    df_numeric = df_copy[numeric_cols].copy()\n    df_non_numeric = df_copy.drop(columns=numeric_cols)\n    \n    # Handle negative values and zeros for certain transformations\n    if choice in [\"log\", \"sqrt\"]:\n        df_numeric = df_numeric.clip(lower=0.001)  # Avoid log(0) and sqrt of negative\n    \n    try:\n        match choice:\n            case \"log\":\n                log_transform = FunctionTransformer(np.log1p, validate=True)\n                transformed_numeric = log_transform.fit_transform(df_numeric)\n            \n            case \"sqrt\":\n                sqrt_transformer = FunctionTransformer(np.sqrt, validate=True)\n                transformed_numeric = sqrt_transformer.fit_transform(df_numeric)\n            \n            case \"reciprocal\":\n                reciprocal_transformer = FunctionTransformer(lambda x: 1 / (x + 0.0001), validate=True)\n                transformed_numeric = reciprocal_transformer.fit_transform(df_numeric)\n            \n            case \"yeo_johnson\":\n                transformed_numeric = power_transform(df_numeric, method='yeo-johnson')\n            \n            case _:\n                print(f\"Unknown choice: {choice}. Returning original data.\")\n                return df\n    \n    except Exception as e:\n        print(f\"Error in transformation {choice}: {e}\")\n        return df\n    \n    # Reconstruct dataframe\n    result = pd.DataFrame(transformed_numeric, columns=numeric_cols, index=df_copy.index)\n    \n    # Add back non-numeric columns\n    if not df_non_numeric.empty:\n        result = pd.concat([result, df_non_numeric.reset_index(drop=True)], axis=1)\n    \n    # Add back target if it existed\n    if target_data is not None:\n        result[target_col] = target_data.reset_index(drop=True)\n    \n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T04:55:43.356350Z","iopub.execute_input":"2025-08-26T04:55:43.356552Z","iopub.status.idle":"2025-08-26T04:55:43.370245Z","shell.execute_reply.started":"2025-08-26T04:55:43.356535Z","shell.execute_reply":"2025-08-26T04:55:43.369543Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# ===== SCALING FUNCTIONS =====\ndef scale_data(df, choice, target_col='sales'):\n    \"\"\"Improved scaling with target preservation\"\"\"\n    df_copy = df.copy()\n    \n    # Separate target if it exists\n    target_data = None\n    if target_col in df_copy.columns:\n        target_data = df_copy[target_col].copy()\n        df_copy = df_copy.drop(columns=[target_col])\n    \n    # Only scale numeric columns\n    numeric_cols = df_copy.select_dtypes(include=[np.number]).columns\n    df_numeric = df_copy[numeric_cols]\n    df_non_numeric = df_copy.drop(columns=numeric_cols)\n    \n    match choice:\n        case \"minmax\":\n            scaler = MinMaxScaler()\n            scaled_numeric = scaler.fit_transform(df_numeric)\n            print(\"Applied MinMaxScaler\")\n        \n        case \"standard\":\n            scaler = StandardScaler()\n            scaled_numeric = scaler.fit_transform(df_numeric)\n            print(\"Applied StandardScaler\")\n        \n        case _:\n            print(\"Invalid scaling type selected!\")\n            return df\n    \n    # Reconstruct dataframe\n    result = pd.DataFrame(scaled_numeric, columns=numeric_cols, index=df_copy.index)\n    \n    # Add back non-numeric columns\n    if not df_non_numeric.empty:\n        result = pd.concat([result, df_non_numeric.reset_index(drop=True)], axis=1)\n    \n    # Add back target\n    if target_data is not None:\n        result[target_col] = target_data.reset_index(drop=True)\n    \n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T04:55:43.370959Z","iopub.execute_input":"2025-08-26T04:55:43.371605Z","iopub.status.idle":"2025-08-26T04:55:43.389738Z","shell.execute_reply.started":"2025-08-26T04:55:43.371586Z","shell.execute_reply":"2025-08-26T04:55:43.389071Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"# ===== MAIN PREPROCESSING PIPELINE =====","metadata":{}},{"cell_type":"code","source":"    anurag = None\n    \"\"\"Complete preprocessing pipeline\"\"\"\n    # Load data\n    # train, test, holiday, oil, store, transaction, sample = load_data()\n    \n    print(\"Original shapes:\")\n    print(f\"Train: {train.shape}, Test: {test.shape}\")\n    \n    # Merge with store information\n    train_merged = train.merge(store, on=\"store_nbr\", how=\"left\")\n    test_merged = test.merge(store, on=\"store_nbr\", how=\"left\")\n    \n    print(f\"After store merge - Train: {train_merged.shape}, Test: {test_merged.shape}\")\n    \n    # Process holidays\n    train_merged = process_holidays(train_merged, holiday)\n    test_merged = process_holidays(test_merged, holiday)\n    \n    # Process oil data\n    train_merged = process_oil_data(train_merged, oil)\n    test_merged = process_oil_data(test_merged, oil)\n    \n    # Process transactions\n    train_merged = process_transactions(train_merged, transaction)\n    test_merged = process_transactions(test_merged, transaction)\n    \n    # Process dates\n    train_processed = modify_date(train_merged)\n    test_processed = modify_date(test_merged)\n    \n    # Drop original date column and id from train\n    train_processed = train_processed.drop(columns=['date'])\n    test_processed = test_processed.drop(columns=['date'])\n    \n    if 'id' in train_processed.columns:\n        train_processed = train_processed.drop(columns=['id'])\n    print(f\"After feature engineering - Train: {train_processed.shape}, Test: {test_processed.shape}\")\n\n    \n    # Handle missing values\n    train_processed = train_processed.fillna(0)  # or use more sophisticated imputation\n    test_processed = test_processed.fillna(0)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T04:55:43.392455Z","iopub.execute_input":"2025-08-26T04:55:43.392631Z","iopub.status.idle":"2025-08-26T04:55:54.831600Z","shell.execute_reply.started":"2025-08-26T04:55:43.392619Z","shell.execute_reply":"2025-08-26T04:55:54.830715Z"}},"outputs":[{"name":"stdout","text":"Original shapes:\nTrain: (3000888, 6), Test: (28512, 5)\nAfter store merge - Train: (3000888, 10), Test: (28512, 9)\nAfter feature engineering - Train: (3008280, 34), Test: (28512, 34)\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"cate_col = [col for col in train_processed.select_dtypes(include= ['object']).columns]\ntrain_processed= train_processed.drop(columns = cate_col)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T04:55:54.832412Z","iopub.execute_input":"2025-08-26T04:55:54.832694Z","iopub.status.idle":"2025-08-26T04:55:55.336169Z","shell.execute_reply.started":"2025-08-26T04:55:54.832662Z","shell.execute_reply":"2025-08-26T04:55:55.335532Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"cate_col = [col for col in test_processed.select_dtypes(include= ['object']).columns]\ntest_processed= test_processed.drop(columns = cate_col)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T04:55:55.336808Z","iopub.execute_input":"2025-08-26T04:55:55.337012Z","iopub.status.idle":"2025-08-26T04:55:55.346839Z","shell.execute_reply.started":"2025-08-26T04:55:55.336997Z","shell.execute_reply":"2025-08-26T04:55:55.346314Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"train_processed.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T04:55:55.347601Z","iopub.execute_input":"2025-08-26T04:55:55.347839Z","iopub.status.idle":"2025-08-26T04:55:55.355391Z","shell.execute_reply.started":"2025-08-26T04:55:55.347813Z","shell.execute_reply":"2025-08-26T04:55:55.354842Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"Index(['store_nbr', 'sales', 'onpromotion', 'cluster', 'is_national_holiday',\n       'is_regional_holiday', 'is_local_holiday', 'is_any_holiday',\n       'dcoilwtico', 'oil_price_ma7', 'oil_price_change', 'transactions',\n       'transactions_ma7', 'year', 'month', 'day', 'dayofweek', 'weekofyear',\n       'is_weekend', 'quarter', 'is_month_start', 'is_month_end',\n       'days_from_start', 'month_sin', 'month_cos', 'dayofweek_sin',\n       'dayofweek_cos'],\n      dtype='object')"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"test_processed = test_processed.drop(columns = ['holiday_national','holiday_regional'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T04:55:55.356061Z","iopub.execute_input":"2025-08-26T04:55:55.356264Z","iopub.status.idle":"2025-08-26T04:55:55.374570Z","shell.execute_reply.started":"2025-08-26T04:55:55.356248Z","shell.execute_reply":"2025-08-26T04:55:55.374009Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"test_processed.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T04:55:55.375206Z","iopub.execute_input":"2025-08-26T04:55:55.375448Z","iopub.status.idle":"2025-08-26T04:55:55.389301Z","shell.execute_reply.started":"2025-08-26T04:55:55.375427Z","shell.execute_reply":"2025-08-26T04:55:55.388713Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"(28512, 27)"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"    # One-hot encode categorical variables\n    # cat_cols = train_processed.select_dtypes(include=['object']).columns\n    # train_processed = onehotencoding(train_processed, cat_cols)\n    # test_processed = onehotencoding(test_processed, cat_cols)\n    # print(f\"After OneHotEncoding - Train: {train_processed.shape}, Test: {test_processed.shape}\")\n    # Align columns between train and test\n    common_cols = list(set(train_processed.columns) & set(test_processed.columns))\n    train_cols_only = [col for col in train_processed.columns if col not in common_cols and col != 'sales']\n    test_cols_only = [col for col in test_processed.columns if col not in common_cols]\n    \n    # Add missing columns with zeros\n    for col in train_cols_only:\n        test_processed[col] = 0\n    for col in test_cols_only:\n        train_processed[col] = 0\n    \n    # Apply transformations (optional - test different options)\n    # train_processed = data_distribution(train_processed, \"log\")  # Uncomment if needed\n    # train_processed = scale_data(train_processed, \"standard\")    # Uncomment if needed\n    \n    print(f\"Final shapes - Train: {train_processed.shape}, Test: {test_processed.shape}\")\n    \n    # return train_processed, test_processed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T04:55:55.389942Z","iopub.execute_input":"2025-08-26T04:55:55.390208Z","iopub.status.idle":"2025-08-26T04:55:55.405917Z","shell.execute_reply.started":"2025-08-26T04:55:55.390187Z","shell.execute_reply":"2025-08-26T04:55:55.405280Z"}},"outputs":[{"name":"stdout","text":"Final shapes - Train: (3008280, 28), Test: (28512, 27)\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"train_final = train_processed\ntest_final = test_processed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T04:55:55.406550Z","iopub.execute_input":"2025-08-26T04:55:55.406739Z","iopub.status.idle":"2025-08-26T04:55:55.423661Z","shell.execute_reply.started":"2025-08-26T04:55:55.406716Z","shell.execute_reply":"2025-08-26T04:55:55.423082Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"# ===== MODEL TRAINING AND EVALUATION =====\ndef evaluate_models(X_train, X_test, y_train, y_test, use_time_series_cv=True):\n    \"\"\"\n    Train and evaluate multiple regression models\n    \"\"\"\n    # Initialize models\n    models = {\n        'LinearRegression': LinearRegression(),\n        'Ridge': Ridge(alpha=1.0),\n        'ElasticNet': ElasticNet(alpha=1.0, l1_ratio=0.5, max_iter=2000),\n        # 'SGDRegressor': SGDRegressor(random_state=42, max_iter=2000),\n        # 'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n        # 'ExtraTrees': ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n        # 'GradientBoosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n        # 'Bagging': BaggingRegressor(random_state=42, n_jobs=-1)\n    }\n    \n    results = {}\n    trained_models = {}\n    \n    print(\"Training and evaluating models...\")\n    print(\"=\" * 60)\n    \n    for name, model in models.items():\n        print(f\"Training {name}...\")\n        \n        try:\n            # Train the model\n            model.fit(X_train, y_train)\n            \n            # Make predictions\n            y_pred_train = model.predict(X_train)\n            y_pred_test = model.predict(X_test)\n            \n            # Calculate metrics\n            train_mse = mean_squared_error(y_train, y_pred_train)\n            test_mse = mean_squared_error(y_test, y_pred_test)\n            train_mae = mean_absolute_error(y_train, y_pred_train)\n            test_mae = mean_absolute_error(y_test, y_pred_test)\n            train_r2 = r2_score(y_train, y_pred_train)\n            test_r2 = r2_score(y_test, y_pred_test)\n            \n            # Time series cross-validation (optional)\n            cv_scores = None\n            if use_time_series_cv:\n                try:\n                    tscv = TimeSeriesSplit(n_splits=5)\n                    cv_scores = cross_val_score(model, X_train, y_train, \n                                              cv=tscv, scoring='neg_mean_squared_error', n_jobs=-1)\n                    cv_rmse = np.sqrt(-cv_scores.mean())\n                except:\n                    cv_rmse = \"N/A\"\n            \n            results[name] = {\n                'train_mse': train_mse,\n                'test_mse': test_mse,\n                'train_rmse': np.sqrt(train_mse),\n                'test_rmse': np.sqrt(test_mse),\n                'train_mae': train_mae,\n                'test_mae': test_mae,\n                'train_r2': train_r2,\n                'test_r2': test_r2,\n                'cv_rmse': cv_rmse if use_time_series_cv else \"N/A\"\n            }\n            \n            trained_models[name] = model\n            \n            print(f\"✓ {name} completed\")\n            print(f\"  Test RMSE: {np.sqrt(test_mse):.4f}\")\n            print(f\"  Test R²: {test_r2:.4f}\")\n            print()\n            \n        except Exception as e:\n            print(f\"✗ Error training {name}: {str(e)}\")\n            results[name] = \"Error\"\n            continue\n    \n    return results, trained_models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T04:55:55.424426Z","iopub.execute_input":"2025-08-26T04:55:55.425144Z","iopub.status.idle":"2025-08-26T04:55:55.442344Z","shell.execute_reply.started":"2025-08-26T04:55:55.425113Z","shell.execute_reply":"2025-08-26T04:55:55.441726Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"def create_ensemble_models(trained_models):\n    \"\"\"\n    Create ensemble models using the best performing individual models\n    \"\"\"\n    # Filter out models that had errors\n    valid_models = {name: model for name, model in trained_models.items() \n                   if model is not None}\n    \n    if len(valid_models) < 2:\n        print(\"Not enough valid models for ensemble creation\")\n        return {}\n    \n    ensemble_models = {}\n    \n    # Voting Regressor (average predictions)\n    try:\n        voting_models = [(name, model) for name, model in valid_models.items() \n                        if name in ['RandomForest', 'ExtraTrees', 'GradientBoosting']]\n        \n        if len(voting_models) >= 2:\n            ensemble_models['VotingRegressor'] = VotingRegressor(\n                estimators=voting_models, n_jobs=-1\n            )\n    except Exception as e:\n        print(f\"Error creating VotingRegressor: {e}\")\n    \n    return ensemble_models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T04:55:55.443100Z","iopub.execute_input":"2025-08-26T04:55:55.443359Z","iopub.status.idle":"2025-08-26T04:55:55.461191Z","shell.execute_reply.started":"2025-08-26T04:55:55.443336Z","shell.execute_reply":"2025-08-26T04:55:55.460549Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"def display_results(results):\n    \"\"\"\n    Display model evaluation results in a formatted table\n    \"\"\"\n    import pandas as pd\n    \n    # Convert results to DataFrame for better display\n    results_df = pd.DataFrame(results).T\n    \n    # Filter out error results\n    results_df = results_df[results_df.apply(lambda x: x != \"Error\").all(axis=1)]\n    \n    if results_df.empty:\n        print(\"No valid results to display\")\n        return\n    \n    # Sort by test RMSE\n    try:\n        results_df = results_df.sort_values('test_rmse', ascending=True)\n    except:\n        pass\n    \n    print(\"Model Performance Summary:\")\n    print(\"=\" * 80)\n    print(f\"{'Model':<20} {'Test RMSE':<12} {'Test R²':<10} {'Test MAE':<12} {'CV RMSE':<12}\")\n    print(\"-\" * 80)\n    \n    for model_name, metrics in results_df.iterrows():\n        try:\n            test_rmse = f\"{metrics['test_rmse']:.4f}\"\n            test_r2 = f\"{metrics['test_r2']:.4f}\"\n            test_mae = f\"{metrics['test_mae']:.4f}\"\n            cv_rmse = f\"{metrics['cv_rmse']:.4f}\" if metrics['cv_rmse'] != \"N/A\" else \"N/A\"\n            \n            print(f\"{model_name:<20} {test_rmse:<12} {test_r2:<10} {test_mae:<12} {cv_rmse:<12}\")\n        except:\n            print(f\"{model_name:<20} Error displaying metrics\")\n    \n    print(\"-\" * 80)\n    \n    # Best model\n    try:\n        best_model = results_df.iloc[0].name\n        best_rmse = results_df.iloc[0]['test_rmse']\n        print(f\"\\nBest Model: {best_model} (Test RMSE: {best_rmse:.4f})\")\n    except:\n        print(\"\\nCould not determine best model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T04:55:55.461933Z","iopub.execute_input":"2025-08-26T04:55:55.462174Z","iopub.status.idle":"2025-08-26T04:55:55.476652Z","shell.execute_reply.started":"2025-08-26T04:55:55.462150Z","shell.execute_reply":"2025-08-26T04:55:55.476163Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"def train_and_evaluate_pipeline(train_data, test_size=0.2, random_state=42):\n    \"\"\"\n    Complete pipeline for training and evaluating models\n    \"\"\"\n    if 'sales' not in train_data.columns:\n        print(\"Error: 'sales' column not found in training data\")\n        return None, None, None\n    \n    # Prepare features and target\n    X = train_data.drop(columns=['sales'])\n    y = train_data['sales']\n    \n    print(f\"Dataset shape: {X.shape}\")\n    print(f\"Target shape: {y.shape}\")\n    print(f\"Target statistics:\")\n    print(y.describe())\n    print()\n    \n    # Split the data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=random_state, shuffle=False  # No shuffle for time series\n    )\n    \n    print(f\"Train set size: {X_train.shape[0]}\")\n    print(f\"Test set size: {X_test.shape[0]}\")\n    print()\n    \n    # Train and evaluate models\n    results, trained_models = evaluate_models(X_train, X_test, y_train, y_test)\n    \n    # Create and train ensemble models\n    ensemble_models = create_ensemble_models(trained_models)\n    \n    if ensemble_models:\n        print(\"Training ensemble models...\")\n        for name, model in ensemble_models.items():\n            try:\n                model.fit(X_train, y_train)\n                y_pred = model.predict(X_test)\n                test_mse = mean_squared_error(y_test, y_pred)\n                test_r2 = r2_score(y_test, y_pred)\n                test_mae = mean_absolute_error(y_test, y_pred)\n                \n                results[name] = {\n                    'train_mse': 0,  # Not calculated for ensemble\n                    'test_mse': test_mse,\n                    'train_rmse': 0,\n                    'test_rmse': np.sqrt(test_mse),\n                    'train_mae': 0,\n                    'test_mae': test_mae,\n                    'train_r2': 0,\n                    'test_r2': test_r2,\n                    'cv_rmse': \"N/A\"\n                }\n                \n                trained_models[name] = model\n                print(f\"✓ {name} completed - Test RMSE: {np.sqrt(test_mse):.4f}\")\n                \n            except Exception as e:\n                print(f\"✗ Error training {name}: {str(e)}\")\n    \n    # Display results\n    print(\"\\n\")\n    display_results(results)\n    \n    return results, trained_models, (X_train, X_test, y_train, y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T04:55:55.477249Z","iopub.execute_input":"2025-08-26T04:55:55.477438Z","iopub.status.idle":"2025-08-26T04:55:55.490686Z","shell.execute_reply.started":"2025-08-26T04:55:55.477424Z","shell.execute_reply":"2025-08-26T04:55:55.490082Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"def make_predictions(model, test_data, feature_columns=None):\n    \"\"\"\n    Make predictions on test data using trained model\n    \"\"\"\n    try:\n        if feature_columns is not None:\n            # Ensure test data has the same columns as training data\n            missing_cols = set(feature_columns) - set(test_data.columns)\n            if missing_cols:\n                print(f\"Warning: Missing columns in test data: {missing_cols}\")\n                for col in missing_cols:\n                    test_data[col] = 0\n            \n            # Select only the required columns in the same order\n            test_data = test_data[feature_columns]\n        \n        predictions = model.predict(test_data)\n        return predictions\n    \n    except Exception as e:\n        print(f\"Error making predictions: {str(e)}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T04:55:55.491327Z","iopub.execute_input":"2025-08-26T04:55:55.491520Z","iopub.status.idle":"2025-08-26T04:55:55.508794Z","shell.execute_reply.started":"2025-08-26T04:55:55.491506Z","shell.execute_reply":"2025-08-26T04:55:55.507910Z"}},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":"# ===== USAGE EXAMPLE =====","metadata":{}},{"cell_type":"code","source":"\n    # Run the preprocessing pipeline\n    # train_final, test_final = preprocess_data()\n    \n    print(\"\\nPreprocessing completed successfully!\")\n    print(f\"Train shape: {train_final.shape}\")\n    print(f\"Test shape: {test_final.shape}\")\n    \n    # Check for missing values\n    print(f\"\\nMissing values in train: {train_final.isnull().sum().sum()}\")\n    print(f\"Missing values in test: {test_final.isnull().sum().sum()}\")\n    \n    # Display basic info\n    print(f\"\\nTrain columns: {train_final.columns.tolist()[:10]}...\")  # First 10 columns\n    print(f\"Target variable stats:\")\n    if 'sales' in train_final.columns:\n        print(train_final['sales'].describe())\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T04:55:55.509565Z","iopub.execute_input":"2025-08-26T04:55:55.509830Z","iopub.status.idle":"2025-08-26T04:55:55.770327Z","shell.execute_reply.started":"2025-08-26T04:55:55.509815Z","shell.execute_reply":"2025-08-26T04:55:55.769638Z"}},"outputs":[{"name":"stdout","text":"\nPreprocessing completed successfully!\nTrain shape: (3008280, 28)\nTest shape: (28512, 27)\n\nMissing values in train: 0\nMissing values in test: 0\n\nTrain columns: ['store_nbr', 'sales', 'onpromotion', 'cluster', 'is_national_holiday', 'is_regional_holiday', 'is_local_holiday', 'is_any_holiday', 'dcoilwtico', 'oil_price_ma7']...\nTarget variable stats:\ncount    3.008280e+06\nmean     3.582643e+02\nstd      1.103486e+03\nmin      0.000000e+00\n25%      0.000000e+00\n50%      1.100000e+01\n75%      1.960000e+02\nmax      1.247170e+05\nName: sales, dtype: float64\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"    # ===== MODEL TRAINING AND EVALUATION =====\n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING MODEL TRAINING AND EVALUATION\")\n    print(\"=\"*60)\n    \n    # Train and evaluate models\n    results, trained_models, data_splits = train_and_evaluate_pipeline(train_final)\n    \n    if results is not None:\n        # Get the best model\n        results_df = pd.DataFrame(results).T\n        results_df = results_df[results_df.apply(lambda x: x != \"Error\").all(axis=1)]\n        \n        if not results_df.empty:\n            best_model_name = results_df.sort_values('test_rmse').index[0]\n            best_model = trained_models[best_model_name]\n            \n            print(f\"\\nBest model selected: {best_model_name}\")\n            \n            # Make predictions on the actual test set\n            print(\"\\nMaking predictions on test set...\")\n            X_train, X_test, y_train, y_test = data_splits\n            feature_columns = X_train.columns.tolist()\n            \n            test_predictions = make_predictions(best_model, test_final, feature_columns)\n            \n            if test_predictions is not None:\n                print(f\"Predictions shape: {test_predictions.shape}\")\n                print(f\"Prediction statistics:\")\n                print(f\"Min: {test_predictions.min():.4f}\")\n                print(f\"Max: {test_predictions.max():.4f}\")\n                print(f\"Mean: {test_predictions.mean():.4f}\")\n                \n                # Create submission file if test data has 'id' column\n                if 'id' in test_final.columns:\n                    submission = pd.DataFrame({\n                        'id': test_final['id'],\n                        'sales': test_predictions\n                    })\n                    \n                    # Save submission file\n                    submission.to_csv('submission.csv', index=False)\n                    print(f\"\\nSubmission file saved as 'submission.csv'\")\n                    print(f\"Submission shape: {submission.shape}\")\n                    print(submission.head())\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n    print(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T04:55:55.771392Z","iopub.execute_input":"2025-08-26T04:55:55.771706Z","iopub.status.idle":"2025-08-26T04:57:51.402260Z","shell.execute_reply.started":"2025-08-26T04:55:55.771681Z","shell.execute_reply":"2025-08-26T04:57:51.401523Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nSTARTING MODEL TRAINING AND EVALUATION\n============================================================\nDataset shape: (3008280, 27)\nTarget shape: (3008280,)\nTarget statistics:\ncount    3.008280e+06\nmean     3.582643e+02\nstd      1.103486e+03\nmin      0.000000e+00\n25%      0.000000e+00\n50%      1.100000e+01\n75%      1.960000e+02\nmax      1.247170e+05\nName: sales, dtype: float64\n\nTrain set size: 2406624\nTest set size: 601656\n\nTraining and evaluating models...\n============================================================\nTraining LinearRegression...\n✓ LinearRegression completed\n  Test RMSE: 1067.7300\n  Test R²: 0.3858\n\nTraining Ridge...\n✓ Ridge completed\n  Test RMSE: 1068.1036\n  Test R²: 0.3854\n\nTraining ElasticNet...\n✓ ElasticNet completed\n  Test RMSE: 1068.0884\n  Test R²: 0.3854\n\n\n\nModel Performance Summary:\n================================================================================\nModel                Test RMSE    Test R²    Test MAE     CV RMSE     \n--------------------------------------------------------------------------------\nLinearRegression     1067.7300    0.3858     522.1726     1006.4971   \nElasticNet           1068.0884    0.3854     510.1100     997.3973    \nRidge                1068.1036    0.3854     524.1863     1006.4925   \n--------------------------------------------------------------------------------\n\nBest Model: LinearRegression (Test RMSE: 1067.7300)\n\nBest model selected: LinearRegression\n\nMaking predictions on test set...\nPredictions shape: (28512,)\nPrediction statistics:\nMin: 68434.0465\nMax: 89649.7058\nMean: 68739.0257\n\nSubmission file saved as 'submission.csv'\nSubmission shape: (28512, 2)\n        id         sales\n0  3000888  68460.695883\n1  3000889  68460.695883\n2  3000890  68526.243175\n3  3000891  69116.168805\n4  3000892  68460.695883\n\n============================================================\nPIPELINE COMPLETED SUCCESSFULLY!\n============================================================\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"submission  = pd.read_csv('/kaggle/working/submission.csv')\nsubmission.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T05:08:52.631171Z","iopub.execute_input":"2025-08-26T05:08:52.631779Z","iopub.status.idle":"2025-08-26T05:08:52.646699Z","shell.execute_reply.started":"2025-08-26T05:08:52.631756Z","shell.execute_reply":"2025-08-26T05:08:52.645936Z"}},"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"(28512, 2)"},"metadata":{}}],"execution_count":56},{"cell_type":"code","source":"train.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T04:57:51.412640Z","iopub.execute_input":"2025-08-26T04:57:51.412936Z","iopub.status.idle":"2025-08-26T04:57:51.434854Z","shell.execute_reply.started":"2025-08-26T04:57:51.412918Z","shell.execute_reply":"2025-08-26T04:57:51.433991Z"}},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"Index(['id', 'date', 'store_nbr', 'family', 'sales', 'onpromotion'], dtype='object')"},"metadata":{}}],"execution_count":52},{"cell_type":"raw","source":"","metadata":{}}]}